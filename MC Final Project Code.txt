##---------------------------------------------------------
## Program for the Coursera Machine Learning -Final Project 
##---------------------------------------------------------
library(caret)
library(rattle)
set.seed(33833)

##---------------------------------------------------------------------
## Read the training data and process it to remove the NA value columns
##---------------------------------------------------------------------
training1<- read.csv("pml-training.csv",sep=",",header=TRUE, na.strings = c("NA", ""))
na_cols <- sapply(training1, function(x) sum(is.na(x)))
na_columns <-names(na_cols[na_cols>0])
training2 <- training1[ , -which(names(training1) %in% na_columns)]
training3 <-training2[ ,2:60]

##----------------------------------------------------
## Split the training data set into train and test data 
##----------------------------------------------------
inTrain <- createDataPartition(y=training$class, p=0.70, list=FALSE)
training <- training3[inTrain,]
testing <- training3[-inTrain,]

##---------------------------------
## Train and predict using Bayesian
##---------------------------------
start.time <- Sys.time()
modelFitBN<- train(classe ~ .,  method = "bayesglm", data = training)
end.time <- Sys.time()
time.taken_BN <- end.time - start.time

resp_BN=predict(modelFitBN, newdata = testing)


##------------------------------------------------
## Train and predict using Recursive Partitioning 
##------------------------------------------------------------
fitControl <- trainControl(## 10-fold Crossvalidation
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 5,
                           verboseIter=FALSE ,
                           # PCA Preprocessing
                           preProcOptions="pca",
                           # With parallel backend
                           allowParallel=TRUE)

start.time <- Sys.time()
modelFitRP <- train(classe ~ ., method = "rpart", data = training)
end.time <- Sys.time()
time.taken_RP <- end.time - start.time

resp_RP=predict(modelFitRP, newdata = testing)

##---------------------------------------
## Train and predict using Random Forest
##---------------------------------------
start.time <- Sys.time()
modelFitRF <- train(classe ~ ., method = "rf", data = training, trControl= fitControl)
end.time <- Sys.time()
time.taken_RF <- end.time - start.time

resp_RF=predict(modelFitRF, newdata = testing)

##------------------------------------------
## Train and predict using Boosting
##------------------------------------------
start.time <- Sys.time()
modelFitBG<- train(classe ~ .,  method = "gbm", data = training, trControl= fitControl)
end.time <- Sys.time()
time.taken_BG <- end.time - start.time

resp_BG=predict(modelFitBG, newdata = testing)


##------------------------------------------
## Train and predict using Bagging Tree
##------------------------------------------
start.time <- Sys.time()
modelFitBT<- train(classe ~ .,  method = "treebag", data = training)
end.time <- Sys.time()
time.taken_BT <- end.time - start.time

resp_BT=predict(modelFitBT, newdata = testing)

##----------------------------
## Obtain the Confusion Matrix
##-----------------------------

ConMatrix_BN<-confusionMatrix(resp_BN, testing$classe)
ConMatrix_RP<-confusionMatrix(resp_RP, testing$classe)
ConMatrix_RF<-confusionMatrix(resp_RF, testing$classe)
ConMatrix_BG<-confusionMatrix(resp_BG, testing$classe)
ConMatrix_BT<-confusionMatrix(resp_BT, testing$classe)

accuracy_BN<-ConMatrix_BN$overall[1]
accuracy_RP<-ConMatrix_RP$overall[1]
accuracy_RF<-ConMatrix_RF$overall[1]
accuracy_BG<-ConMatrix_BG$overall[1]
accuracy_BT<-ConMatrix_BT$overall[1]

##----------------------------------------
## Identify and plot the top influential predictors
##----------------------------------------

varImpRP<-varImp(modelFitRP)
varImpRF<-varImp(modelFitRF)
varImpBG<-varImp(modelFitBG)
varImpBT<-varImp(modelFitBT)

plot(varImpRP, main = "Top 10 influencial Predictors - Recursive Partioning", top = 10)
plot(varImpRF, main = "Top 10 influencial Predictors - Random Forest", top = 10)
plot(varImpBG, main = "Top 10 influencial Predictors - Boosting", top = 10)
plot(varImpBT, main = "Top 10 influencial Predictors - Treebag", top = 10)

##-----------------------
## Plot the Decision Tree
##------------------------
fancyRpartPlot(modelFitRP$finalModel)
fancyRpartPlot(modelFitRF$finalModel)
fancyRpartPlot(modelFitBG$finalModel)
fancyRpartPlot(modelFitBT$finalModel)
##-----------------------------------------------------------------
## Read the 20 test cases and process them to remove the NA value columns
##-----------------------------------------------------------------

test1<- read.csv("pml-testing.csv",sep=",",header=TRUE, na.strings = c("NA", ""))
na_cols <- sapply(test1, function(x) sum(is.na(x)))
na_columns <-names(na_cols[na_cols>0])
test2 <- test1[ , -which(names(training1) %in% na_columns)]
test<-test2[ ,2:60]
##-----------------------------------------
## Predict the output for the 20 test cases
##-----------------------------------------

resp_test_BN=predict(modelFitBN, newdata = test)
resp_test_RP=predict(modelFitRP, newdata = test)
resp_test_RF=predict(modelFitRF, newdata = test)
resp_test_BG=predict(modelFitBG, newdata = test)
resp_test_BT=predict(modelFitBT, newdata = test)


